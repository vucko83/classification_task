{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_features_range(n_samples=100, m_features=3, m_n_ratio=1):\n",
    "    '''\n",
    "    Used for definition of algorithm parameters based on available number of features\n",
    "     and samples, after feature selection.\n",
    "     \n",
    "    It is used for definition of grid search params through a pipeline\n",
    "     \n",
    "    :param n_samples: number of samples in the dataframe/matrix\n",
    "    :param m_features: number of features after feature selection\n",
    "    :param m_n_ratio: maximal number of features relative to number of samples\n",
    "    \n",
    "    :return: range of numbers of features used for param optimization in algorithms \n",
    "    '''\n",
    "\n",
    "    max_features = int(round(n_samples / m_n_ratio))\n",
    "    if m_features < max_features:\n",
    "        max_features = m_features\n",
    "\n",
    "    if m_features == 1:\n",
    "        min_features = 1\n",
    "    else:\n",
    "        min_features = 2\n",
    "\n",
    "    step_size = int(round(np.log2(max_features)))\n",
    "\n",
    "    param_range = [a for a in range(min_features, max_features, step_size)]\n",
    "\n",
    "    if param_range[-1] < max_features:\n",
    "        param_range.append(max_features)\n",
    "\n",
    "    return param_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Normalization\n",
    "'''\n",
    "df=pd.read_csv('~/Downloads/Data-Classification.txt')\n",
    "\n",
    "\n",
    "scaler=MinMaxScaler() # initialize scaler\n",
    "X_norm=scaler.fit_transform(X)\n",
    "pd.DataFrame(X_norm).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve as prc\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Prepare Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "df=pd.read_csv('~/Downloads/Data-Classification.txt')\n",
    "\n",
    "# Create Train and Test Splits\n",
    "X=df.iloc[:,1:] \n",
    "y=df.iloc[:,0] # Separate input and output\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'C', 'B'], dtype=object)"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Models and Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "benchmark_algorithms ={\n",
    "    'LR' : LogisticRegression(),\n",
    "    'RF' : RandomForestClassifier(),\n",
    "    'KNN': KNeighborsClassifier()\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def mine_roc_auc(y_true, y_pred, average='micro'):\n",
    "    '''\n",
    "    Wrapper for AUC type measures (auroc and auprc),\n",
    "    because they demand input in form of dummy variables\n",
    "    '''\n",
    "    y_true_dummy = pd.get_dummies(y_true)\n",
    "    y_pred_dummy = pd.get_dummies(y_pred)\n",
    "    return (roc_auc_score(y_true_dummy, y_pred_dummy, average=average))\n",
    "\n",
    "# Scoring dict to be used in all evaluations\n",
    "scoring = {\n",
    "           'au_prc_macro': make_scorer(mine_roc_auc, average='macro'),\n",
    "           'au_prc_micro': make_scorer(mine_roc_auc, average='macro'),\n",
    "           'f1_macro': 'f1_macro',\n",
    "           'f1_micro': 'f1_micro',\n",
    "           'precision_macro': 'precision_macro',\n",
    "           'precision_micro': 'precision_micro',\n",
    "           'accuracy': 'accuracy',\n",
    "          }\n",
    "\n",
    "###!!!!!! Micro Macro Explanation https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin/16001\n",
    "### !!! plotting cross val cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "#from sklearn.metrics import recall_score\n",
    "\n",
    "models_dict = {}\n",
    "scores_dict = {}\n",
    "\n",
    "for model_name, model in benchmark_algorithms.items():\n",
    "    \n",
    "    scores = cross_validate(model, X_train, y_train, scoring=scoring,\n",
    "                            cv=5, return_train_score=True)\n",
    "    scores_aggregated= { measure: {'avg': '%.3f' % round(np.average(values),3),\\\n",
    "                                   'std': '%.2f' % round(np.std(values),2)} \\\n",
    "                                    for measure, values in scores.items()}\n",
    "    scores_dict.update({model_name:scores_aggregated})\n",
    "    \n",
    "    fitted_model = model.fit(X_train, y_train)\n",
    "    models_dict.update({model_name:fitted_model})\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusions={}\n",
    "for name, model in models_dict.items():\n",
    "    y_predicted = model.predict(X_test)\n",
    "    confusions.update({name: confusion_matrix(y_predicted, y_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[109,   1,   5],\n",
       "       [  0, 108,   1],\n",
       "       [  4,   0, 102]])"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusions['LR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KNN': {'fit_time': {'avg': '0.003', 'std': '0.00'},\n",
       "  'score_time': {'avg': '0.041', 'std': '0.00'},\n",
       "  'test_accuracy': {'avg': '0.394', 'std': '0.04'},\n",
       "  'test_au_prc_macro': {'avg': '0.546', 'std': '0.03'},\n",
       "  'test_au_prc_micro': {'avg': '0.546', 'std': '0.03'},\n",
       "  'test_f1_macro': {'avg': '0.388', 'std': '0.05'},\n",
       "  'test_f1_micro': {'avg': '0.394', 'std': '0.04'},\n",
       "  'test_precision_macro': {'avg': '0.397', 'std': '0.04'},\n",
       "  'test_precision_micro': {'avg': '0.394', 'std': '0.04'},\n",
       "  'train_accuracy': {'avg': '0.616', 'std': '0.01'},\n",
       "  'train_au_prc_macro': {'avg': '0.712', 'std': '0.01'},\n",
       "  'train_au_prc_micro': {'avg': '0.712', 'std': '0.01'},\n",
       "  'train_f1_macro': {'avg': '0.612', 'std': '0.01'},\n",
       "  'train_f1_micro': {'avg': '0.616', 'std': '0.01'},\n",
       "  'train_precision_macro': {'avg': '0.628', 'std': '0.01'},\n",
       "  'train_precision_micro': {'avg': '0.616', 'std': '0.01'}},\n",
       " 'LR': {'fit_time': {'avg': '0.102', 'std': '0.01'},\n",
       "  'score_time': {'avg': '0.013', 'std': '0.00'},\n",
       "  'test_accuracy': {'avg': '0.946', 'std': '0.02'},\n",
       "  'test_au_prc_macro': {'avg': '0.960', 'std': '0.01'},\n",
       "  'test_au_prc_micro': {'avg': '0.960', 'std': '0.01'},\n",
       "  'test_f1_macro': {'avg': '0.946', 'std': '0.02'},\n",
       "  'test_f1_micro': {'avg': '0.946', 'std': '0.02'},\n",
       "  'test_precision_macro': {'avg': '0.947', 'std': '0.02'},\n",
       "  'test_precision_micro': {'avg': '0.946', 'std': '0.02'},\n",
       "  'train_accuracy': {'avg': '0.983', 'std': '0.01'},\n",
       "  'train_au_prc_macro': {'avg': '0.987', 'std': '0.00'},\n",
       "  'train_au_prc_micro': {'avg': '0.987', 'std': '0.00'},\n",
       "  'train_f1_macro': {'avg': '0.983', 'std': '0.01'},\n",
       "  'train_f1_micro': {'avg': '0.983', 'std': '0.01'},\n",
       "  'train_precision_macro': {'avg': '0.983', 'std': '0.01'},\n",
       "  'train_precision_micro': {'avg': '0.983', 'std': '0.01'}},\n",
       " 'RF': {'fit_time': {'avg': '0.034', 'std': '0.00'},\n",
       "  'score_time': {'avg': '0.021', 'std': '0.00'},\n",
       "  'test_accuracy': {'avg': '0.899', 'std': '0.04'},\n",
       "  'test_au_prc_macro': {'avg': '0.924', 'std': '0.03'},\n",
       "  'test_au_prc_micro': {'avg': '0.924', 'std': '0.03'},\n",
       "  'test_f1_macro': {'avg': '0.899', 'std': '0.04'},\n",
       "  'test_f1_micro': {'avg': '0.899', 'std': '0.04'},\n",
       "  'test_precision_macro': {'avg': '0.902', 'std': '0.04'},\n",
       "  'test_precision_micro': {'avg': '0.899', 'std': '0.04'},\n",
       "  'train_accuracy': {'avg': '0.998', 'std': '0.00'},\n",
       "  'train_au_prc_macro': {'avg': '0.998', 'std': '0.00'},\n",
       "  'train_au_prc_micro': {'avg': '0.998', 'std': '0.00'},\n",
       "  'train_f1_macro': {'avg': '0.998', 'std': '0.00'},\n",
       "  'train_f1_micro': {'avg': '0.998', 'std': '0.00'},\n",
       "  'train_precision_macro': {'avg': '0.998', 'std': '0.00'},\n",
       "  'train_precision_micro': {'avg': '0.998', 'std': '0.00'}}}"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection and Parameter Optimization\n",
    "\n",
    "Since benchmark model (Logistic regression) gave good results in terms of different validation measure (both on training and test data), further analyses will be focused on finding:\n",
    "- More interpretable solutions\n",
    "- More robust solutions (in terms of generalization)\n",
    "\n",
    "For this purpose, hyper-parameters of models will be optimized as well as feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check if it can be done with simple Pipeline()\n",
    "pipe = Pipeline([\n",
    "    ('reduce_dim', PCA()),\n",
    "    ('classify', RidgeClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ridge_param_dict(name='classify', estimators=[LogisticRegression()], n_samples=1000, m_features=15): # Check this if needed\n",
    "    dict={\n",
    "        name: estimators,\n",
    "        name + '__' + 'penalty': ['l1', 'l2'],\n",
    "        name + '__' + 'C': [0.01, 0.03, 0.1, 0.5, 0.8],\n",
    "        name + '__' + 'max_iter': [10000],\n",
    "        name + '__' + 'solver': ['saga']\n",
    "        # check for multinomial\n",
    "    }\n",
    "    return (dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rf_param_dict(name='classify', estimators=[RandomForestClassifier()], n_samples=1000, m_features=15):\n",
    "    dict={\n",
    "        name: estimators,\n",
    "        name + '__' + 'n_estimators': range(20, 101, 20),\n",
    "        name + '__' + 'max_features': n_features_range(n_samples, m_features),\n",
    "        name + '__' + 'min_samples_leaf': np.arange(0.01, 0.03, 0.05),\n",
    "        name + '__' + 'max_depth': range(2, 11, 2)\n",
    "    }\n",
    "    return (dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_params_pca_nmf(name='reduce_dim', reducers=[PCA()], n_samples=100, m_features=[5, 10, 15, 20, 25, 30], funcs=[]):\n",
    "    params=[]\n",
    "\n",
    "    for func in funcs.values():\n",
    "        for m in m_features:\n",
    "            dict = {\n",
    "                name: reducers,\n",
    "                name+'__'+'n_components':[m]\n",
    "            }\n",
    "            dict.update(func(m_features=m))\n",
    "\n",
    "            params.append(dict.copy())\n",
    "    return (params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "algorithms={'Ridge':ridge_param_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pars=create_params_pca_nmf(name='reduce_dim', reducers=[PCA()], n_samples=100, m_features=[5, 10, 15, 20, 25, 30], funcs=algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y_train=y_train.map({'A':0, 'B':1, 'C':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_train=y_train.map({0:'A', 1:'B', 2:'C'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search=GridSearchCV(estimator = pipe, param_grid = pars, scoring=scoring, refit='au_prc_macro', cv=5, n_jobs=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "components=search.cv_results_\n",
    "p=search.cv_results_['params'][0] # params for one algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV log analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_df=pd.DataFrame(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_df.sort_values('mean_test_au_prc_macro', ascending=False).to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classify': LogisticRegression(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=10000, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l1', random_state=None, solver='saga', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " 'classify__C': 0.01,\n",
       " 'classify__max_iter': 10000,\n",
       " 'classify__penalty': 'l1',\n",
       " 'classify__solver': 'saga',\n",
       " 'reduce_dim': PCA(copy=True, iterated_power='auto', n_components=30, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       " 'reduce_dim__n_components': 5}"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df.iloc[0]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00005194,  0.00005775,  0.00016196,  0.00001041,  0.00029534,\n",
       "         0.00125713,  0.00056716,  0.00038789,  0.00204169,  0.01705677,\n",
       "         0.00222576,  0.0235838 ,  0.00325135,  0.00382209,  0.00051098,\n",
       "         0.01400537,  0.0038145 ,  0.00454965,  0.02261119,  0.00778988,\n",
       "         0.02423947,  0.05169558,  0.06984935,  0.12177898,  0.1468223 ,\n",
       "         0.05872214,  0.17039034,  0.04522326,  0.00033544,  0.14801178],\n",
       "       [ 0.00009962,  0.00006927,  0.00024747,  0.00052031,  0.00004521,\n",
       "         0.00001639,  0.00120692,  0.00159266,  0.00353399,  0.03443111,\n",
       "         0.0047229 ,  0.00420293,  0.00556044,  0.01104581,  0.01371967,\n",
       "         0.00893658,  0.00030236,  0.00496319,  0.07590543,  0.00567939,\n",
       "         0.08957671,  0.0030803 ,  0.09665548,  0.04637347,  0.13899926,\n",
       "         0.05623149,  0.00851108,  0.01989131,  0.04998539,  0.06065147],\n",
       "       [ 0.00023764,  0.00019664,  0.00009421,  0.00034618,  0.00002847,\n",
       "         0.00061626,  0.00033779,  0.00044141,  0.00072825,  0.00824063,\n",
       "         0.00527222,  0.02574342,  0.00074389,  0.00383013,  0.01006432,\n",
       "         0.01924962,  0.00322954,  0.00036062,  0.08116719,  0.01667625,\n",
       "         0.12025905,  0.05407255,  0.01571183,  0.08131114,  0.23306458,\n",
       "         0.00291514,  0.08164278,  0.00354891,  0.06864422,  0.01604547]])"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "np.abs(search.best_estimator_.named_steps['classify'].coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=30, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('classify', LogisticRegression(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=10000, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='saga', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_estimator_.named_steps['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('classify', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.set_params(**components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C', 'C', 'C', 'C', 'B', 'C', 'C', 'C', 'B', 'C', 'C', 'C', 'B',\n",
       "       'A', 'B', 'C', 'B', 'C', 'A', 'C', 'B', 'B', 'A', 'B', 'B', 'C',\n",
       "       'C', 'A', 'C', 'C', 'C', 'A', 'B', 'C', 'B', 'C', 'A', 'C', 'C',\n",
       "       'B', 'B', 'B', 'B', 'A', 'C', 'A', 'B', 'B', 'B', 'B', 'C', 'C',\n",
       "       'C', 'B', 'B', 'C', 'C', 'C', 'C', 'B', 'C', 'A', 'A', 'C', 'C',\n",
       "       'C', 'C', 'A', 'C', 'C', 'A', 'C', 'C', 'C', 'C', 'C', 'C', 'A',\n",
       "       'C', 'C', 'C', 'C', 'C', 'C', 'B', 'A', 'C', 'C', 'C', 'B', 'C',\n",
       "       'C', 'C', 'C', 'B', 'B', 'C', 'B', 'C', 'A', 'A', 'B', 'C', 'C',\n",
       "       'A', 'C', 'C', 'B', 'C', 'C', 'A', 'C', 'C', 'C', 'C', 'C', 'C',\n",
       "       'C', 'A', 'A', 'C', 'B', 'C', 'B', 'C', 'C', 'C', 'B', 'C', 'C',\n",
       "       'C', 'C', 'A', 'B', 'A', 'A', 'B', 'B', 'B', 'A', 'B', 'B', 'C',\n",
       "       'B', 'C', 'A', 'B', 'C', 'C', 'C', 'B', 'C', 'C', 'B', 'A', 'C',\n",
       "       'C', 'B', 'A', 'B', 'C', 'C', 'C', 'C', 'A', 'C', 'C', 'B', 'C',\n",
       "       'C', 'B', 'C', 'B', 'A', 'C', 'B', 'C', 'C', 'C', 'C', 'A', 'C',\n",
       "       'C', 'A', 'A', 'A', 'B', 'C', 'A', 'C', 'C', 'C', 'B', 'C', 'C',\n",
       "       'B', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'A', 'C', 'C', 'A', 'C',\n",
       "       'A', 'A', 'C', 'C', 'C', 'C', 'C', 'A', 'B', 'C', 'A', 'A', 'B',\n",
       "       'C', 'C', 'C', 'A', 'C', 'B', 'B', 'B', 'C', 'B', 'B', 'C', 'B',\n",
       "       'C', 'C', 'C', 'B', 'C', 'C', 'B', 'C', 'A', 'A', 'C', 'B', 'A',\n",
       "       'C', 'C', 'C', 'B', 'C', 'A', 'A', 'B', 'C', 'B', 'C', 'A', 'C',\n",
       "       'A', 'C', 'C', 'B', 'B', 'C', 'C', 'C', 'C', 'A', 'B', 'C', 'A',\n",
       "       'C', 'B', 'C', 'C', 'B', 'C', 'B', 'C', 'C', 'C', 'C', 'C', 'C',\n",
       "       'B', 'C', 'B', 'B', 'C', 'C', 'C', 'B', 'C', 'C', 'B', 'B', 'A',\n",
       "       'B', 'C', 'C', 'C', 'C', 'C', 'C', 'A', 'B', 'C', 'B', 'A', 'B',\n",
       "       'C', 'B', 'A', 'C', 'C', 'C', 'C', 'A', 'C', 'B', 'B', 'A', 'C',\n",
       "       'B', 'C', 'A', 'C', 'C'], dtype=object)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search=GridSearchCV(estimator = LogisticRegression(), param_grid = grid, cv=5, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=2,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': [0.01, 0.1, 0.3]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 2.4281404 ,  0.08468533,  5.56339583,  0.10868721,  7.47109303,\n",
       "         0.13296876]),\n",
       " 'mean_score_time': array([ 0.02096758,  0.00084143,  0.00067897,  0.00078926,  0.00077062,\n",
       "         0.00096245]),\n",
       " 'mean_test_score': array([ 0.89552239,  0.94477612,  0.94179104,  0.95522388,  0.94179104,\n",
       "         0.94925373]),\n",
       " 'mean_train_score': array([ 0.91642546,  0.9682898 ,  0.95746681,  0.97948315,  0.9597077 ,\n",
       "         0.98134813]),\n",
       " 'param_C': masked_array(data = [0.01 0.01 0.1 0.1 0.3 0.3],\n",
       "              mask = [False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_penalty': masked_array(data = ['l1' 'l2' 'l1' 'l2' 'l1' 'l2'],\n",
       "              mask = [False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'C': 0.01, 'penalty': 'l1'},\n",
       "  {'C': 0.01, 'penalty': 'l2'},\n",
       "  {'C': 0.1, 'penalty': 'l1'},\n",
       "  {'C': 0.1, 'penalty': 'l2'},\n",
       "  {'C': 0.3, 'penalty': 'l1'},\n",
       "  {'C': 0.3, 'penalty': 'l2'}],\n",
       " 'rank_test_score': array([6, 3, 4, 1, 4, 2], dtype=int32),\n",
       " 'split0_test_score': array([ 0.87407407,  0.9037037 ,  0.9037037 ,  0.92592593,  0.91111111,\n",
       "         0.91111111]),\n",
       " 'split0_train_score': array([ 0.93271028,  0.97570093,  0.96448598,  0.98691589,  0.9682243 ,\n",
       "         0.98691589]),\n",
       " 'split1_test_score': array([ 0.89552239,  0.95522388,  0.95522388,  0.97014925,  0.94776119,\n",
       "         0.97014925]),\n",
       " 'split1_train_score': array([ 0.91791045,  0.96455224,  0.95708955,  0.97761194,  0.9608209 ,\n",
       "         0.98320896]),\n",
       " 'split2_test_score': array([ 0.92537313,  0.97014925,  0.97014925,  0.95522388,  0.97014925,\n",
       "         0.95522388]),\n",
       " 'split2_train_score': array([ 0.90671642,  0.96641791,  0.95149254,  0.97574627,  0.95149254,\n",
       "         0.97574627]),\n",
       " 'split3_test_score': array([ 0.90298507,  0.94029851,  0.93283582,  0.94776119,  0.93283582,\n",
       "         0.94776119]),\n",
       " 'split3_train_score': array([ 0.91231343,  0.97574627,  0.9608209 ,  0.98507463,  0.96641791,\n",
       "         0.9869403 ]),\n",
       " 'split4_test_score': array([ 0.87969925,  0.95488722,  0.94736842,  0.97744361,  0.94736842,\n",
       "         0.96240602]),\n",
       " 'split4_train_score': array([ 0.91247672,  0.95903166,  0.95344507,  0.97206704,  0.95158287,\n",
       "         0.97392924]),\n",
       " 'std_fit_time': array([ 0.47582837,  0.00765761,  1.09540219,  0.01094007,  0.99112411,\n",
       "         0.00868281]),\n",
       " 'std_score_time': array([  3.83293445e-02,   1.41892061e-04,   3.01017596e-05,\n",
       "          1.27583892e-04,   2.39811045e-04,   1.67423658e-04]),\n",
       " 'std_test_score': array([ 0.01821387,  0.02268922,  0.02261491,  0.01807355,  0.0194894 ,\n",
       "         0.02055189]),\n",
       " 'std_train_score': array([ 0.00887882,  0.00653774,  0.00474399,  0.0056387 ,  0.00710353,\n",
       "         0.00551644])}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression(**a).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression(**a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dum_true = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dum_pred = pd.get_dummies(lr_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97500000000000009"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(dum_true, dum_pred, average='micro', sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#AUPRC in multilabel settings\n",
    "#http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot precision recall curves for multiclass\n",
    "\"\"\"\n",
    "\n",
    "from itertools import cycle\n",
    "# setup plot details\n",
    "colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n",
    "\n",
    "plt.figure(figsize=(7, 8))\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.2f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.2f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.25)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
